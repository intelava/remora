{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remora Speedup Benchmark\n",
    "\n",
    "This notebook demonstrates remora's Triton-accelerated VLM inference, comparing:\n",
    "\n",
    "1. **Stock PyTorch** - baseline nn.Linear layers\n",
    "2. **W8A16 Surgery** - int8 weights with fp16 activations (2x memory bandwidth savings)\n",
    "3. **Full VLM Surgery** - W8A16 + fused vision projector\n",
    "\n",
    "We also showcase **JaggedTensor** support for variable-length sequences without padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Uncomment to install dependencies if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install remora with VLMEval extras.\n",
    "# !pip install -e \".[eval]\"\n",
    "# Or minimal install without extras:\n",
    "# !pip install -e .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model (Stock PyTorch Baseline)\n",
    "\n",
    "First, we load SmolVLM-Base without any optimizations to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from remora.models import load_model_and_tokenizer\n",
    "from remora import is_triton_available\n",
    "\n",
    "# Configuration\n",
    "PRESET = \"smolvlm-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WARMUP_RUNS = 2\n",
    "BENCHMARK_RUNS = 5\n",
    "MAX_NEW_TOKENS = 32\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Triton available: {is_triton_available()}\")\n",
    "print(f\"\\nLoading {PRESET}...\")\n",
    "\n",
    "# Load the model (we'll clone it for different experiments)\n",
    "model, tokenizer = load_model_and_tokenizer(PRESET, device=DEVICE)\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Helper\n",
    "\n",
    "Define a function to measure generation time and tokens per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(model, tokenizer, prompt, num_runs=5, warmup=2, max_new_tokens=32):\n",
    "    \"\"\"Benchmark model generation, returning avg time and tokens/sec.\"\"\"\n",
    "    # Prepare inputs\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warmup runs\n",
    "    for _ in range(warmup):\n",
    "        with torch.inference_mode():\n",
    "            _ = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "    \n",
    "    # Synchronize before timing\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    total_tokens = 0\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            output = model.generate(**inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "        total_tokens += output.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    avg_tokens = total_tokens / len(times)\n",
    "    tps = avg_tokens / avg_time\n",
    "    \n",
    "    return {\n",
    "        \"avg_time_ms\": avg_time * 1000,\n",
    "        \"tokens_per_sec\": tps,\n",
    "        \"avg_new_tokens\": avg_tokens,\n",
    "    }\n",
    "\n",
    "# Test prompt\n",
    "PROMPT = \"Explain the benefits of quantized inference in three sentences.\"\n",
    "print(f\"Prompt: {PROMPT[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Stock PyTorch (Baseline)\n",
    "\n",
    "Run generation with the unmodified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running stock PyTorch baseline...\")\n",
    "stock_results = benchmark_generation(\n",
    "    model, tokenizer, PROMPT, \n",
    "    num_runs=BENCHMARK_RUNS, warmup=WARMUP_RUNS, max_new_tokens=MAX_NEW_TOKENS\n",
    ")\n",
    "print(f\"  Avg time: {stock_results['avg_time_ms']:.1f} ms\")\n",
    "print(f\"  Tokens/sec: {stock_results['tokens_per_sec']:.1f}\")\n",
    "print(f\"  New tokens: {stock_results['avg_new_tokens']:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Remora W8A16 Surgery\n",
    "\n",
    "Apply W8A16 quantization (int8 weights, fp16 activations) to all Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from remora import hijack_model, is_triton_available\n",
    "from remora.surgery import TritonBitLinear\n",
    "\n",
    "print(\"Applying W8A16 surgery to all Linear layers...\")\n",
    "num_replaced = hijack_model(model, verbose=False)\n",
    "\n",
    "# Diagnostic: Check a TritonBitLinear layer\n",
    "triton_layers = [m for m in model.modules() if isinstance(m, TritonBitLinear)]\n",
    "if triton_layers:\n",
    "    layer = triton_layers[0]\n",
    "    print(f\"  Found {len(triton_layers)} TritonBitLinear layers\")\n",
    "    print(f\"  Sample layer weight device: {layer.weight.device}\")\n",
    "    print(f\"  Sample layer weight_int8 device: {layer.weight_int8.device}\")\n",
    "    print(f\"  Triton available: {is_triton_available()}\")\n",
    "    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "print(\"\\nRunning W8A16 benchmark...\")\n",
    "w8a16_results = benchmark_generation(\n",
    "    model, tokenizer, PROMPT,\n",
    "    num_runs=BENCHMARK_RUNS, warmup=WARMUP_RUNS, max_new_tokens=MAX_NEW_TOKENS\n",
    ")\n",
    "print(f\"  Avg time: {w8a16_results['avg_time_ms']:.1f} ms\")\n",
    "print(f\"  Tokens/sec: {w8a16_results['tokens_per_sec']:.1f}\")\n",
    "\n",
    "speedup = stock_results['avg_time_ms'] / w8a16_results['avg_time_ms']\n",
    "print(f\"\\n  ‚ö° Speedup vs stock: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e6af7",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Compare all benchmarks side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9359f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {PRESET}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Max new tokens: {MAX_NEW_TOKENS}\")\n",
    "print()\n",
    "\n",
    "results = [\n",
    "    (\"Stock PyTorch\", stock_results),\n",
    "    (\"W8A16 Surgery\", w8a16_results),\n",
    "]\n",
    "\n",
    "print(f\"{'Method':<20} {'Time (ms)':<12} {'Tokens/s':<12} {'Speedup':<10}\")\n",
    "print(\"-\" * 54)\n",
    "\n",
    "baseline_time = stock_results['avg_time_ms']\n",
    "for name, r in results:\n",
    "    speedup = baseline_time / r['avg_time_ms']\n",
    "    speedup_str = f\"{speedup:.2f}x\" if name != \"Stock PyTorch\" else \"baseline\"\n",
    "    print(f\"{name:<20} {r['avg_time_ms']:<12.1f} {r['tokens_per_sec']:<12.1f} {speedup_str:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0655a",
   "metadata": {},
   "source": [
    "## Bonus: JaggedTensor Demo\n",
    "\n",
    "Remora supports variable-length sequences without padding using `JaggedTensor`. This is useful for mixed image+text batches where padding wastes compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4322241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remora import JaggedTensor, pack_sequences, unpack_sequences, pad_jagged\n",
    "\n",
    "# Simulate 3 sequences of different lengths (like image + varying text)\n",
    "seq1 = torch.randn(576, 64, device=DEVICE)   # 576 image tokens\n",
    "seq2 = torch.randn(128, 64, device=DEVICE)   # 128 text tokens  \n",
    "seq3 = torch.randn(256, 64, device=DEVICE)   # 256 text tokens\n",
    "\n",
    "# Pack into JaggedTensor - NO PADDING!\n",
    "jagged = pack_sequences([seq1, seq2, seq3])\n",
    "\n",
    "print(\"JaggedTensor properties:\")\n",
    "print(f\"  Total tokens: {jagged.total_tokens} (vs {576 * 3}={576*3} if padded)\")\n",
    "print(f\"  cu_seqlens: {jagged.cu_seqlens.tolist()}\")\n",
    "print(f\"  Batch size: {jagged.batch_size}\")\n",
    "print(f\"  Memory saved: {((576*3 - jagged.total_tokens) * 64 * 2) / 1024:.1f} KB\")\n",
    "\n",
    "# Can apply W8A16 operations directly on jagged data\n",
    "from remora import w8a16_gemm, quantize_weight_per_channel\n",
    "\n",
    "# Create a test projection\n",
    "W = torch.randn(128, 64, device=DEVICE, dtype=torch.float16)\n",
    "w_int8, scales = quantize_weight_per_channel(W)\n",
    "\n",
    "# Apply to all tokens at once - no padding needed!\n",
    "out = w8a16_gemm(jagged.data, w_int8, scales)\n",
    "print(f\"\\nProjected jagged data: {jagged.data.shape} -> {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f894c",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Remora provides several optimizations beyond `torch.compile`:\n",
    "\n",
    "| Feature | Description | torch.compile? |\n",
    "|---------|-------------|----------------|\n",
    "| **W8A16 GEMM** | int8 weights, fp16 activations | ‚ùå Struggles with int8 |\n",
    "| **JaggedTensor** | No padding for variable sequences | ‚ùå Recompiles on shape change |\n",
    "| **Fused GELU+Linear** | Activation fused with matmul | ‚ö†Ô∏è Unreliable |\n",
    "| **Vision Projector** | Fused 2-layer MLP for VLMs | ‚ùå Breaks on shape transitions |\n",
    "\n",
    "For full VLM optimization including vision projector fusion:\n",
    "```python\n",
    "from remora import full_vlm_surgery\n",
    "full_vlm_surgery(model)  # Replaces projector + all Linear layers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c353b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model, tokenizer\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Done! üéâ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
